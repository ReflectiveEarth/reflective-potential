{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df351ca8",
   "metadata": {},
   "source": [
    "# Data Ingest\n",
    "\n",
    "This notebook contains a workflow for downloading meteorological data from the European Centre for Medium-Range Weather Forecasts (ECMWF) Copernicus [Climate Data Store](https://cds.climate.copernicus.eu/) and uploading to a Google Cloud Storage bucket.\n",
    "\n",
    "In this particular case, we want to ingest ECMWF 5th Reanalysis (ERA5) hourly data over a 30 year interval. Making a single request would be prohibitively large, so we break the request up by day. This data ingest is intended to support an analysis of Earth's radiation budget, so we will request four variables to serve as boundary conditions:\n",
    "\n",
    "* Top of Atmosphere Incident Solar Radiation (`toa_incident_solar_radiation`)\n",
    "* Top of Atmosphere Net Solar Radiation (`top_net_solar_radiation`)\n",
    "* Downwelling Solar Radiation at the Surface (`surface_solar_radiation_downwards`)\n",
    "* Net Solar Radiation at the Surface (`surface_net_solar_radiation`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd15804e",
   "metadata": {},
   "source": [
    "## Preliminaries\n",
    "\n",
    "This notebook depends on several python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab140d1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q cdsapi joblib tqdm urllib3 certifi gsutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fa93fc7",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b4866d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import timedelta, date\n",
    "import multiprocessing\n",
    "\n",
    "import cdsapi\n",
    "import certifi\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import trange, tqdm\n",
    "import urllib3\n",
    "\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',\n",
    "    ca_certs=certifi.where()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dba1f176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil version: 4.62\n",
      "checksum: PACKAGED_GSUTIL_INSTALLS_DO_NOT_HAVE_CHECKSUMS (!= fe14a00285d4702ed626050d0f9ae955)\n",
      "boto version: 2.49.0\n",
      "python version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) [GCC 9.3.0]\n",
      "OS: Linux 4.19.0-16-cloud-amd64\n",
      "multiprocessing available: True\n",
      "using cloud sdk: False\n",
      "pass cloud sdk credentials to gsutil: False\n",
      "config path(s): /etc/boto.cfg, /home/jupyter/.boto\n",
      "gsutil path: /opt/conda/bin/gsutil\n",
      "compiled crcmod: True\n",
      "installed via package manager: True\n",
      "editable install: False\n"
     ]
    }
   ],
   "source": [
    "!gsutil version -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5975c249",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd9de5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "      start_date: date object to start from.\n",
    "      end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "      date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in trange(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def ingest_cds_gcp(single_date, bucket):\n",
    "    \"\"\"Retrieve data from Copernicus Data Service and upload to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket: Google Cloud Storage bucket to upload to.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; uploads data to Google Cloud Storage as side effect.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client(progress=False, quiet=True)\n",
    "    c.retrieve(\n",
    "    \"reanalysis-era5-single-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\n",
    "            \"surface_solar_radiation_downwards\", \"toa_incident_solar_radiation\",\n",
    "            \"surface_net_solar_radiation\", \"top_net_solar_radiation\"\n",
    "        ],\n",
    "        \"year\": single_date.strftime(\"%Y\"),\n",
    "        \"month\": single_date.strftime(\"%m\"),\n",
    "        \"day\": single_date.strftime(\"%d\"),\n",
    "        \"time\": [\n",
    "            \"00:00\", \"01:00\", \"02:00\",\n",
    "            \"03:00\", \"04:00\", \"05:00\",\n",
    "            \"06:00\", \"07:00\", \"08:00\",\n",
    "            \"09:00\", \"10:00\", \"11:00\",\n",
    "            \"12:00\", \"13:00\", \"14:00\",\n",
    "            \"15:00\", \"16:00\", \"17:00\",\n",
    "            \"18:00\", \"19:00\", \"20:00\",\n",
    "            \"21:00\", \"22:00\", \"23:00\",\n",
    "        ],\n",
    "        \"format\": \"netcdf\",\n",
    "    },\n",
    "    f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    os.system(f\"gsutil -m cp -r {single_date.strftime('%Y%m%d')}.nc \\\n",
    "                gs://{bucket}/ && rm {single_date.strftime('%Y%m%d')}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296907b2",
   "metadata": {},
   "source": [
    "## Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17d9c781",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8051f056eab049608a0d7065a782acef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "start_date = date(1991, 1, 24)\n",
    "end_date = date(1991, 2, 1)\n",
    "bucket = \"era5-single-level\"\n",
    "\n",
    "result = Parallel(n_jobs=-2,  # use all but one CPU\n",
    "                  backend=\"multiprocessing\")(delayed(ingest_cds_gcp)(day, bucket) \n",
    "                                             for day in daterange(start_date, end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e963f49",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In the next notebook, `02-Processing.ipynb` we'll demonstrate the steps necessary to distill the ingested hourly data into daily-mean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c94d33f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
