{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16437ced",
   "metadata": {},
   "source": [
    "# Data Processing\n",
    "\n",
    "This notebook contains a workflow to:\n",
    "1. Download hourly ERA5 data from a Google Cloud Storage bucket.\n",
    "1. Process the hourly ERA5 data into daily ERA5 data.\n",
    "1. Upload the daily data to a Google Cloud Storage bucket.\n",
    "\n",
    "This data processing is necessary to support an analysis of Earth's radiation budget based on daily solar fluxes at the surface and top of atmosphere, so we will process hourly averages into daily averages. ERA5 does not output top of atmosphere outgoing solar radiation or upwelling solar radiation at the surface, however these quantities can be calculated using the available fluxes at those levels (e.g. incoming radiation and net radiation at the top of atmosphere).\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* A Google Cloud project with Cloud Storage enabled\n",
    "* The following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ee6365c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q gsutil tqdm xarray scipy dask netCDF4 pandas joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10934645",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8580c484",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from datetime import timedelta, date, datetime\n",
    "from functools import wraps\n",
    "import multiprocessing\n",
    "from os import system\n",
    "from sys import platform\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c507b94",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Our analysis seeks a long-term estimate of the amount of outgoing radiation that Earth's surface can reflect. ERA5 has 42 years of hourly data available. A long-term climatology is typically defined as 30 years. Thus, we ingest the latest 30 years: 1991 through 2020. Since making a single request would be prohibitively large, we break the request up by day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "542e76aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(1990, 1, 1)\n",
    "end_date = date(2021, 1, 1)\n",
    "hourly_data_bucket = \"era5-single-level\"\n",
    "daily_data_bucket = \"era5-single-level-daily\"\n",
    "n_jobs = 1  # number of jobs for parallelization; if 1, then serial; if negative, then (n_cpus + 1 + n_jobs) are used\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "if platform == \"darwin\":\n",
    "    multiprocessing.set_start_method(\"fork\", force=True)  # ipython bug workaround https://github.com/ipython/ipython/issues/12396"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1510c262",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "45e60c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def retry(exceptions, tries=4, delay=3, backoff=2, logger=None):\n",
    "    \"\"\"Retry calling the decorated function using an exponential backoff.\n",
    "\n",
    "    Args:\n",
    "        exceptions: The exception to check. may be a tuple of\n",
    "            exceptions to check.\n",
    "        tries: Number of times to try (not retry) before giving up.\n",
    "        delay: Initial delay between retries in seconds.\n",
    "        backoff: Backoff multiplier (e.g. value of 2 will double the delay\n",
    "            each retry).\n",
    "        logger: Logger to use. If None, print.\n",
    "    \"\"\"\n",
    "    def deco_retry(f):\n",
    "\n",
    "        @wraps(f)\n",
    "        def f_retry(*args, **kwargs):\n",
    "            mtries, mdelay = tries, delay\n",
    "            while mtries > 1:\n",
    "                try:\n",
    "                    return f(*args, **kwargs)\n",
    "                except exceptions as e:\n",
    "                    msg = '{}, Retrying in {} seconds...'.format(e, mdelay)\n",
    "                    if logger:\n",
    "                        logger.warning(msg)\n",
    "                    else:\n",
    "                        print(msg)\n",
    "                    time.sleep(mdelay)\n",
    "                    mtries -= 1\n",
    "                    mdelay *= backoff\n",
    "            return f(*args, **kwargs)\n",
    "\n",
    "        return f_retry  # true decorator\n",
    "\n",
    "    return deco_retry\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date: date object to start from.\n",
    "        end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "        date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "@retry(RuntimeError, backoff=1)\n",
    "def get_date_data_gcs(single_date, bucket):\n",
    "    \"\"\"Download a dataset for a single date from Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket: Google Cloud Storage bucket to download from.\n",
    "        \n",
    "    Returns:\n",
    "        Exit status of system call to get data.\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: if system call exit status is not 0.\n",
    "    \"\"\"\n",
    "    download_status = system(f\"gsutil -m cp -r gs://{bucket}/{single_date.strftime('%Y%m%d')}.nc \\\n",
    "                             ./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    if download_status != 0:\n",
    "        raise RuntimeError(\"Non-zero exit status\")\n",
    "\n",
    "    return download_status\n",
    "\n",
    "\n",
    "@retry(RuntimeError, backoff=1)\n",
    "def put_date_data_gcs(single_date, bucket, cleanup=False):\n",
    "    \"\"\"Upload a dataset for a single date to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket: Google Cloud Storage bucket to download from.\n",
    "        cleanup: Optionally remove the file locally.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; downloads data from Google Cloud Storage as side effect.\n",
    "        \n",
    "    Raises:\n",
    "        RuntimeError: if system call exit status is not 0.\n",
    "    \"\"\"\n",
    "    upload_status = system(f\"gsutil -m cp -r ./{single_date.strftime('%Y%m%d')}.nc gs://{bucket}/\")\n",
    "\n",
    "    if upload_status != 0:\n",
    "        raise RuntimeError(\"Non-zero exit status\")\n",
    "    \n",
    "    if (upload_status == 0) & cleanup:\n",
    "        system(f\"rm {single_date.strftime('%Y%m%d')}.nc\")\n",
    "\n",
    "    return upload_status\n",
    "\n",
    "\n",
    "def modify_units(dataset, starting_units, ending_units, conversion_factor):\n",
    "    \"\"\"Modify the units of a variable.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset\n",
    "        starting_units: str of units to be modified\n",
    "        ending_units: str of units after modification\n",
    "        conversion_factor: numerical factor to apply to convert units\n",
    "    \n",
    "    Returns:\n",
    "        xarray Dataset with units modified for variables with units matching the starting unit.\n",
    "    \"\"\"\n",
    "    for variable in dataset:\n",
    "        if dataset[variable].attrs[\"units\"] == starting_units:\n",
    "            dataset[variable] = dataset[variable] * conversion_factor\n",
    "            dataset[variable].attrs[\"units\"] = ending_units\n",
    "    return dataset\n",
    "\n",
    "        \n",
    "def compute_daily_average(dataset):\n",
    "    \"\"\"Compute the daily average and an input xarray dataset.\"\"\"\n",
    "    return dataset.resample(time='1D').sum() / dataset.sizes[\"time\"]\n",
    "\n",
    "\n",
    "def compute_boundary_fluxes(dataset):\n",
    "    \"\"\"Compute missing boundary fluxes at the surface and top of atmosphere if possible.\n",
    "    \n",
    "    Use available radiative fluxes e.g. net solar radiation and incoming solar radiation to \n",
    "    compute outgoing solar radiation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset with radiative fluxes at the surface and top of atmosphere.\n",
    "        \n",
    "    Returns:\n",
    "        xarray Dataset with missing fluxes at the boundaries.\n",
    "    \"\"\"\n",
    "    if (\"tosr\" not in dataset) and all(x in dataset for x in [\"tisr\", \"tsr\"]):\n",
    "        dataset[\"tosr\"] = dataset[\"tisr\"] - dataset[\"tsr\"]\n",
    "        dataset[\"tosr\"].attrs[\"long_name\"] = \"TOA outgoing solar radiation\"\n",
    "        dataset[\"tosr\"].attrs[\"standard_name\"] = \"toa_outgoing_shortwave_flux\"\n",
    "    if (\"ssru\" not in dataset) and all(x in dataset for x in [\"ssrd\", \"ssr\"]):\n",
    "        dataset[\"ssru\"] = dataset[\"ssrd\"] - dataset[\"ssr\"]\n",
    "        dataset[\"ssru\"].attrs[\"long_name\"] = \"Surface solar radiation upwards\"\n",
    "        dataset[\"ssru\"].attrs[\"standard_name\"] = \"surface_upwelling_shortwave_flux_in_air\"\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def drop_unneccesary_variables(dataset, keep_vars):\n",
    "    \"\"\"Drop variables not specified as necessary.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset.\n",
    "        keep_vars: list of variables to keep.\n",
    "    \n",
    "    Returns:\n",
    "        xarray Dataset with specified variables.\n",
    "    \"\"\"\n",
    "    drop_vars = list(set(dataset.data_vars).symmetric_difference(set(keep_vars)))\n",
    "    return dataset.drop_vars(drop_vars)\n",
    "\n",
    "\n",
    "def process_hourly_data(single_date, hourly_data_bucket, daily_data_bucket, cleanup=False):\n",
    "    \"\"\"Process hourly average data into daily average data.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        hourly_data_bucket: str name of Google Cloud Storage bucket for hourly data.\n",
    "        daily_data_bucket: str name of Google Cloud Storage bucket for daily data.\n",
    "        cleanup: boolean option to remove downloaded data after processing.\n",
    "    \n",
    "    Returns:\n",
    "        Exit status of system call to upload processed data to Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    download_status = get_date_data_gcs(single_date, hourly_data_bucket)\n",
    "    \n",
    "    with xr.open_dataset(f\"./{single_date.strftime('%Y%m%d')}.nc\") as ds:\n",
    "        ds = compute_boundary_fluxes(ds)\n",
    "        ds = drop_unneccesary_variables(ds, keep_vars=[\"ssrd\", \"ssru\", \"tisr\", \"tosr\"])\n",
    "        ds = compute_daily_average(ds)\n",
    "        ds = modify_units(ds, \"J m**-2\", \"W m**-2\", (1 / 3600))  # 3600 seconds in an hour\n",
    "        ds.to_netcdf(f\"./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "\n",
    "    upload_status = put_date_data_gcs(single_date, daily_data_bucket, cleanup)\n",
    "    return upload_status"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5143465b-8e86-4674-b160-31a7d9c32409",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "For each day between the specified start and end dates, download the hourly data, process it, and upload it to a bucket for daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13838f39-2b96-4c93-a28a-279876ad42aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93cac576fb5c4a5b86ca54a013cb6a7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm_joblib(tqdm(total=sum(1 for _ in daterange(start_date, end_date)))) as pbar:\n",
    "    Parallel(n_jobs=n_jobs,\n",
    "             backend=\"multiprocessing\")(delayed(process_hourly_data)(day, hourly_data_bucket, daily_data_bucket, cleanup=True)\n",
    "                                        for day in daterange(start_date, end_date))"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
