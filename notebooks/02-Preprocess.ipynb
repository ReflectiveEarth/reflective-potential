{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68fcee49",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "This notebook contains a workflow to:\n",
    "1. Download hourly ERA5 data from a Google Cloud Storage bucket,\n",
    "1. Process the hourly ERA5 data into daily ERA5 data with the fields necessary for our subsequent analysis, and\n",
    "1. Upload the daily data to a Google Cloud Storage bucket.\n",
    "\n",
    "This data processing is necessary to support an analysis of Earth's radiation budget based on daily solar fluxes at the surface and the top of atmosphere, so we will process hourly averages into daily averages. ERA5 does not output top of atmosphere outgoing solar radiation or upwelling solar radiation at the surface, however these quantities can be inferred using the available fluxes at those levels (e.g. incoming radiation and net radiation at the top of atmosphere).\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* A Google Cloud project with Cloud Storage enabled ([Create new account](https://cloud.google.com/))\n",
    "* The following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9a4e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -q tqdm xarray dask netCDF4 joblib google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3a3f3b5",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7cf4215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from datetime import timedelta, date\n",
    "import logging\n",
    "import multiprocessing\n",
    "from os import system, path\n",
    "from sys import platform\n",
    "import warnings\n",
    "\n",
    "from google.cloud import storage\n",
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "260fb3d1",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Our analysis seeks a long-term estimate of the amount of outgoing radiation that Earth's surface can reflect. ERA5 has 42 years of hourly data available. A long-term climatology is typically defined as 30 years. Thus, we ingest the latest 30 years: 1991 through 2020. Since making a single request would be prohibitively large, we break the request up by day. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4852b2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_year = 1991\n",
    "end_year = 2020\n",
    "hourly_data_bucket = \"era5-single-level\"\n",
    "daily_data_bucket = \"era5-single-level-daily\"\n",
    "annual_data_bucket = \"era5-single-level-annual\"\n",
    "n_jobs = -3  # number of jobs for parallelization; if 1, then serial; if negative, then (n_cpus + 1 + n_jobs) are used\n",
    "\n",
    "# Xarray configuration\n",
    "xr.set_options(keep_attrs=True)\n",
    "\n",
    "# Multiprocessing configuration for MacOS\n",
    "if platform == \"darwin\":\n",
    "    multiprocessing.set_start_method(\"fork\", force=True)  # ipython bug workaround https://github.com/ipython/ipython/issues/12396\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(filename=\"process.log\", filemode=\"w\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4528ef",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a666552f",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date: date object to start from.\n",
    "        end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "        date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def get_data_gcs(file_name, bucket_name):\n",
    "    \"\"\"Download a dataset for a single date from Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        file_name: file_name to download from gcs.\n",
    "        bucket_name: Google Cloud Storage bucket to download from.\n",
    "    \n",
    "    Returns:\n",
    "        Nothing; downloads data from Google Cloud Storage as a side effect.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.download_to_filename(filename=file_name)\n",
    "\n",
    "\n",
    "def put_data_gcs(file_name, bucket_name):\n",
    "    \"\"\"Upload a dataset for a single date to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        file_name: name of file to upload to gcs.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; uploads data to Google Cloud Storage as a side effect.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.blob(file_name)\n",
    "    blob.upload_from_filename(filename=file_name)\n",
    "    \n",
    "\n",
    "def check_blob_size(single_date, bucket_name, raise_threshold=1):\n",
    "    \"\"\"Verify that a GCS blob is larger than a specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        raise_threshold: file size below which an exception should be raised.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.get_blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    if blob.size < raise_threshold:\n",
    "        raise Exception(f\"{single_date.strftime('%Y%m%d')} data file size is smaller than expected\")\n",
    "    else:\n",
    "        logging.info(f\"{single_date.strftime('%Y%m%d')} file size in GCS is {int(blob.size * 1e-6)}MB\")\n",
    "\n",
    "\n",
    "def modify_units(dataset, starting_units, ending_units, conversion_factor):\n",
    "    \"\"\"Modify the units of a variable.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset\n",
    "        starting_units: str of units to be modified\n",
    "        ending_units: str of units after modification\n",
    "        conversion_factor: numerical factor to apply to convert units\n",
    "    \n",
    "    Returns:\n",
    "        xarray Dataset with units modified for variables with units matching the starting unit.\n",
    "    \"\"\"\n",
    "    for variable in dataset:\n",
    "        if dataset[variable].attrs[\"units\"] == starting_units:\n",
    "            dataset[variable] = dataset[variable] * conversion_factor\n",
    "            dataset[variable].attrs[\"units\"] = ending_units\n",
    "    return dataset\n",
    "\n",
    "        \n",
    "def compute_daily_average(dataset):\n",
    "    \"\"\"Compute the daily average and an input xarray dataset.\"\"\"\n",
    "    return dataset.resample(time='1D').sum() / dataset.sizes[\"time\"]\n",
    "\n",
    "\n",
    "def compute_boundary_fluxes(dataset):\n",
    "    \"\"\"Compute missing boundary fluxes at the surface and top of atmosphere if possible.\n",
    "    \n",
    "    Use available radiative fluxes e.g. net solar radiation and incoming solar radiation to \n",
    "    compute outgoing solar radiation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset with radiative fluxes at the surface and top of atmosphere.\n",
    "        \n",
    "    Returns:\n",
    "        xarray Dataset with missing fluxes at the boundaries.\n",
    "    \"\"\"\n",
    "    if (\"tosr\" not in dataset) and all(x in dataset for x in [\"tisr\", \"tsr\"]):\n",
    "        dataset[\"tosr\"] = dataset[\"tisr\"] - dataset[\"tsr\"]\n",
    "        dataset[\"tosr\"].attrs[\"long_name\"] = \"TOA outgoing solar radiation\"\n",
    "        dataset[\"tosr\"].attrs[\"standard_name\"] = \"toa_outgoing_shortwave_flux\"\n",
    "    if (\"ssru\" not in dataset) and all(x in dataset for x in [\"ssrd\", \"ssr\"]):\n",
    "        dataset[\"ssru\"] = dataset[\"ssrd\"] - dataset[\"ssr\"]\n",
    "        dataset[\"ssru\"].attrs[\"long_name\"] = \"Surface solar radiation upwards\"\n",
    "        dataset[\"ssru\"].attrs[\"standard_name\"] = \"surface_upwelling_shortwave_flux_in_air\"\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def drop_unneccesary_variables(dataset, keep_vars):\n",
    "    \"\"\"Drop variables not specified as necessary.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset.\n",
    "        keep_vars: list of variables to keep.\n",
    "    \n",
    "    Returns:\n",
    "        xarray Dataset with specified variables.\n",
    "    \"\"\"\n",
    "    drop_vars = list(set(dataset.data_vars).symmetric_difference(set(keep_vars)))\n",
    "    return dataset.drop_vars(drop_vars)\n",
    "\n",
    "\n",
    "def preprocess_hourly_data(single_date, hourly_data_bucket, daily_data_bucket, cleanup=False):\n",
    "    \"\"\"Process hourly average data into daily average data.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        hourly_data_bucket: str name of Google Cloud Storage bucket for hourly data.\n",
    "        daily_data_bucket: str name of Google Cloud Storage bucket for daily data.\n",
    "        cleanup: boolean option to remove downloaded data after processing.\n",
    "    \n",
    "    Returns:\n",
    "        Exit status of system call to upload processed data to Google Cloud Storage.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: if the same bucket is provided for both hourly and daily data.\n",
    "    \"\"\"\n",
    "    if hourly_data_bucket == daily_data_bucket:\n",
    "        raise Exception(\"You must provide different buckets for hourly and daily data.\")\n",
    "    \n",
    "    get_data_gcs(file_name=f\"{single_date.strftime('%Y%m%d')}.nc\", bucket_name=hourly_data_bucket)\n",
    "    \n",
    "    with xr.open_dataset(f\"{single_date.strftime('%Y%m%d')}.nc\") as ds:\n",
    "        ds = compute_boundary_fluxes(ds)\n",
    "        ds = compute_daily_average(ds)\n",
    "        ds = drop_unneccesary_variables(ds, keep_vars=[\"ssrd\", \"ssru\", \"tisr\", \"tosr\"])\n",
    "        ds = modify_units(ds, \"J m**-2\", \"W m**-2\", (1 / 3600))  # 3600 seconds in an hour\n",
    "        ds.to_netcdf(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "\n",
    "    put_data_gcs(file_name=f\"{single_date.strftime('%Y%m%d')}.nc\", bucket_name=daily_data_bucket)\n",
    "    check_blob_size(single_date, daily_data_bucket, raise_threshold=1e+7)\n",
    "    \n",
    "    if cleanup:\n",
    "        system(f\"rm {single_date.strftime('%Y%m%d')}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf02fe1f",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "For each day between the specified start and end dates, download the hourly data, process it, and upload it to a different bucket for daily data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2e0e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year in range(start_year, end_year + 1):\n",
    "    \n",
    "    start_date = date(year, 1, 1)\n",
    "    end_date = date(year + 1, 1, 1)\n",
    "\n",
    "    with tqdm_joblib(tqdm(total=sum(1 for _ in daterange(start_date, end_date)))) as pbar:\n",
    "        Parallel(n_jobs=n_jobs,\n",
    "                 backend=\"multiprocessing\")(delayed(preprocess_hourly_data)(day, \n",
    "                                                                            hourly_data_bucket, \n",
    "                                                                            daily_data_bucket, \n",
    "                                                                            cleanup=False)\n",
    "                                            for day in daterange(start_date, end_date))\n",
    "    \n",
    "    ds = xr.open_mfdataset(\"*.nc\", parallel=True)\n",
    "    ds.mean(dim=\"time\").compute().assign_coords(time=year).expand_dims(\"time\").to_netcdf(f\"{year}.nc\")\n",
    "    \n",
    "    put_data_gcs(file_name=f\"{year}.nc\", bucket_name=annual_data_bucket)\n",
    "    \n",
    "    system(f\"rm *.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8562a1",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In this notebook, we preprocessed ERA5 data by computing radiative fluxes at the atmospheric boundaries (surface, top of atmosphere) and averaging hourly-means to daily-means. The resulting daily-mean fields were uploaded to their own Google Cloud Storage bucket in netCDF files by date and by year. In the next notebook, `03-Analyze.ipynb` we'll analyze the annual averages using a simple model of reflected radiation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3b24cec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m73",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m73"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
