{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb6331d2",
   "metadata": {},
   "source": [
    "# Data Ingest\n",
    "\n",
    "This notebook contains a workflow to:\n",
    "1. Download meteorological reanalysis data from the Copernicus [Climate Data Store](https://cds.climate.copernicus.eu/) and\n",
    "2. Upload the data to a Google Cloud Storage bucket.\n",
    "\n",
    "This data ingest is necessary to support an analysis of Earth's radiation budget, so we will request four available variables to serve as boundary conditions:\n",
    "\n",
    "* Top of Atmosphere Incident Solar Radiation (`toa_incident_solar_radiation`)\n",
    "* Top of Atmosphere Net Solar Radiation (`top_net_solar_radiation`)\n",
    "* Downwelling Solar Radiation at the Surface (`surface_solar_radiation_downwards`)\n",
    "* Net Solar Radiation at the Surface (`surface_net_solar_radiation`)\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* A Copernicus Climate Data Store account ([Create new account](https://cds.climate.copernicus.eu/user/register))\n",
    "* A Google Cloud project with Cloud Storage enabled ([Create new account](https://cloud.google.com/))\n",
    "* Python packages. See `environments` directory for platform specific environment files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671f4e7",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cea01a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import check_environment\n",
    "\n",
    "check_environment(\"ingest\")\n",
    "\n",
    "import contextlib\n",
    "from datetime import timedelta, date\n",
    "import logging\n",
    "import multiprocessing\n",
    "from os import environ\n",
    "import os\n",
    "from sys import platform\n",
    "import urllib.request\n",
    "\n",
    "import cdsapi\n",
    "import certifi\n",
    "from google.cloud import storage\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bfdc594",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Our analysis seeks a long-term estimate of the amount of outgoing radiation that Earth's surface can reflect. ERA5 has 42 years of hourly data available. A long-term climatology is typically defined as 30 years. Thus, we ingest the latest 30 years: 1991 through 2020. Since making a single request would be prohibitively large, we break the request up by day. \n",
    "\n",
    "Our ingest workflow downloads ECMWF Copernicus Climate Data Store (CDS) files locally, uploads them to a Google Cloud Storage bucket, and removes the local file. We define a GCS bucket here, although a user may use whatever bucket they choose in their Google Cloud project.\n",
    "\n",
    "The CDS Client API must be configured prior to use ([How to use the CDS API](https://cds.climate.copernicus.eu/api-how-to)). On initialization `cdsapi.Client()` looks for a `url` and `key` in environment variables, in a `.cdsapirc` file, or in the class input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7b2167",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(2010, 1, 1)\n",
    "end_date = date(2011, 1, 1)\n",
    "hourly_data_bucket = \"era5-single-level\"\n",
    "n_jobs = -3  # number of jobs for parallelization; if 1, then serial; if negative, then (n_cpus + 1 + n_jobs) are used\n",
    "\n",
    "# ECMWF C3S CDS Client Configuration\n",
    "try:\n",
    "    if None not in (environ[\"CDSAPI_URL\"], environ[\"CDSAPI_KEY\"]):\n",
    "        pass\n",
    "except KeyError:\n",
    "    # Fallback to .cdsapirc\n",
    "    try:\n",
    "        with open(os.path.join(os.path.expanduser(\"~\"),\".cdsapirc\"), \"r\") as f:\n",
    "            output = f.read()\n",
    "    except IOError:\n",
    "        logging.warning(\"No $CDSAPI_URL, $CDSAPI_KEY, or .cdsapirc found.\")\n",
    "\n",
    "# Certificate management\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',\n",
    "    ca_certs=certifi.where()\n",
    ")\n",
    "\n",
    "# Logging configuration\n",
    "logging.basicConfig(filename=\"ingest.log\", filemode=\"w\", level=logging.INFO)\n",
    "\n",
    "# Multiprocessing configuration for MacOS\n",
    "if platform == \"darwin\":\n",
    "    multiprocessing.set_start_method(\"fork\", force=True)  # ipython bug workaround https://github.com/ipython/ipython/issues/12396\n",
    "    \n",
    "# Project ID\n",
    "url = \"http://metadata.google.internal/computeMetadata/v1/project/project-id\"\n",
    "req = urllib.request.Request(url)\n",
    "req.add_header(\"Metadata-Flavor\", \"Google\")\n",
    "project_id = urllib.request.urlopen(req).read().decode()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "485b58d5",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfce880",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "      start_date: date object to start from.\n",
    "      end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "      date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def put_date_data_gcs(single_date, bucket_name, user_project=None):\n",
    "    \"\"\"Upload a dataset for a single date to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to download from.\n",
    "        user_project: project ID for requester pays billing.\n",
    "\n",
    "    Returns:\n",
    "        Nothing; uploads data to Google Cloud Storage.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name, user_project=user_project)    \n",
    "    blob = bucket.blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    blob.upload_from_filename(filename=f\"./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "\n",
    "def check_blob_size(single_date, bucket_name, user_project=None, raise_threshold=1e+2):\n",
    "    \"\"\"Verify that a GCS blob is larger than a specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        user_project: project ID for requester pays billing.\n",
    "        raise_threshold: file size below which an exception should be raised.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; logs an info message about the size of the blob.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: if the blob file size is less than the specified threshold\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name, user_project=user_project)    \n",
    "    blob = bucket.get_blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    if blob.size < raise_threshold:\n",
    "        raise Exception(f\"{single_date.strftime('%Y%m%d')} data file size is smaller than expected\")\n",
    "    else:\n",
    "        logging.info(f\"{single_date.strftime('%Y%m%d')} file size in GCS is {int(blob.size * 1e-6)}MB\")\n",
    "\n",
    "\n",
    "def ingest_cds_to_gcs(single_date, bucket_name, user_project=None, cleanup=False):\n",
    "    \"\"\"Retrieve data from Copernicus Data Service and upload data to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        user_project: project ID for requester pays billing.\n",
    "        cleanup: Optionally remove the downloaded CDS data after upload to GCS.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; downloads a file, uploads it to GCS, and optionally removes it.\n",
    "    \"\"\"\n",
    "    client = cdsapi.Client(progress=False, quiet=True)\n",
    "    client.retrieve(\n",
    "    \"reanalysis-era5-single-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\n",
    "            \"surface_solar_radiation_downwards\", \"toa_incident_solar_radiation\",\n",
    "            \"surface_net_solar_radiation\", \"top_net_solar_radiation\"\n",
    "        ],\n",
    "        \"year\": single_date.strftime(\"%Y\"),\n",
    "        \"month\": single_date.strftime(\"%m\"),\n",
    "        \"day\": single_date.strftime(\"%d\"),\n",
    "        \"time\": [\n",
    "            \"00:00\", \"01:00\", \"02:00\",\n",
    "            \"03:00\", \"04:00\", \"05:00\",\n",
    "            \"06:00\", \"07:00\", \"08:00\",\n",
    "            \"09:00\", \"10:00\", \"11:00\",\n",
    "            \"12:00\", \"13:00\", \"14:00\",\n",
    "            \"15:00\", \"16:00\", \"17:00\",\n",
    "            \"18:00\", \"19:00\", \"20:00\",\n",
    "            \"21:00\", \"22:00\", \"23:00\",\n",
    "        ],\n",
    "        \"format\": \"netcdf\",\n",
    "    },\n",
    "    f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "    put_date_data_gcs(single_date, bucket_name, user_project=user_project)\n",
    "    check_blob_size(single_date, bucket_name, user_project=user_project, raise_threshold=1.8e+8)\n",
    "    \n",
    "    if cleanup:\n",
    "        os.remove(f\"{single_date.strftime('%Y%m%d')}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c10d1fc3",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "For each day between the specified start and end dates, make a request for hourly data containing 4 variables, download the data locally, and upload it to Google Cloud Storage. A `tqdm` progress bar tracks completion of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "998a537b",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm_joblib(tqdm(total=sum(1 for _ in daterange(start_date, end_date)))) as pbar:\n",
    "    Parallel(n_jobs=n_jobs,\n",
    "             backend=\"multiprocessing\")(delayed(ingest_cds_to_gcs)(day, \n",
    "                                                                   hourly_data_bucket,\n",
    "                                                                   user_project=project_id,\n",
    "                                                                   cleanup=True)\n",
    "                                        for day in daterange(start_date, end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0abdd712",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In the next notebook, `02-Preprocess.ipynb` we'll demonstrate the steps necessary to process the ingested hourly-mean data into daily-mean data with all the necessary variables in our preferred units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637a72a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m81",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m81"
  },
  "kernelspec": {
   "display_name": "Python [conda env:ingest] *",
   "language": "python",
   "name": "conda-env-ingest-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
