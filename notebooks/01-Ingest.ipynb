{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "402652be",
   "metadata": {},
   "source": [
    "# Data Ingest\n",
    "\n",
    "This notebook contains a workflow to:\n",
    "1. Download meteorological reanalysis data from the Copernicus [Climate Data Store](https://cds.climate.copernicus.eu/) and\n",
    "2. Upload the data to a Google Cloud Storage bucket.\n",
    "\n",
    "This data ingest is necessary to support an analysis of Earth's radiation budget, so we will request four variables to serve as boundary conditions:\n",
    "\n",
    "* Top of Atmosphere Incident Solar Radiation (`toa_incident_solar_radiation`)\n",
    "* Top of Atmosphere Net Solar Radiation (`top_net_solar_radiation`)\n",
    "* Downwelling Solar Radiation at the Surface (`surface_solar_radiation_downwards`)\n",
    "* Net Solar Radiation at the Surface (`surface_net_solar_radiation`)\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* A Copernicus Climate Data Store account ([Create new account](https://cds.climate.copernicus.eu/user/register))\n",
    "* A Google Cloud project with Cloud Storage enabled\n",
    "* The following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ad1b4c53",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q cdsapi joblib tqdm urllib3 certifi gsutil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b892098",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "46844d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ, path, system\n",
    "from datetime import timedelta, date\n",
    "\n",
    "import cdsapi\n",
    "import certifi\n",
    "import contextlib\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a92ac4",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Our analysis seeks a long-term estimate of the amount of outgoing radiation that Earth's surface can reflect. ERA5 has 42 years of hourly data available. A long-term climatology is typically defined as 30 years. Thus, we ingest the latest 30 years: 1991 through 2020. Since making a single request would be prohibitively large, we break the request up by day. \n",
    "\n",
    "Our ingest workflow downloads ECMWF Copernicus Climate Data Store (CDS) files locally, uploads them to a Google Cloud Storage bucket, and removes the local file. We define a GCS bucket here, although a user may use whatever bucket they choose in their Google Cloud project.\n",
    "\n",
    "The CDS Client API must be configured prior to use ([How to use the CDS API](https://cds.climate.copernicus.eu/api-how-to)). On initialization `cdsapi.Client()` looks for a `url` and `key` in environment variables, in a `.cdsapirc` file, or in the class input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21695a93",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(1991, 5, 1)\n",
    "end_date = date(1991, 6, 1)\n",
    "hourly_data_bucket = \"era5-single-level\"\n",
    "\n",
    "# ECMWF C3S CDS Client Configuration\n",
    "try:\n",
    "    if None not in (environ[\"CDSAPI_URL\"], environ[\"CDSAPI_KEY\"]):\n",
    "        pass\n",
    "except KeyError:\n",
    "    # Fallback to .cdsapirc\n",
    "    try:\n",
    "        with open(path.join(path.expanduser(\"~\"),\".cdsapirc\"), \"r\") as f:\n",
    "            output = f.read()\n",
    "    except IOError:\n",
    "        logging.warning(\"No $CDSAPI_URL, $CDSAPI_KEY, or .cdsapirc found.\")\n",
    "\n",
    "# Certificate management\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',\n",
    "    ca_certs=certifi.where()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa137bec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gsutil version: 4.62\n",
      "checksum: PACKAGED_GSUTIL_INSTALLS_DO_NOT_HAVE_CHECKSUMS (!= fe14a00285d4702ed626050d0f9ae955)\n",
      "boto version: 2.49.0\n",
      "python version: 3.7.10 | packaged by conda-forge | (default, Feb 19 2021, 16:07:37) [GCC 9.3.0]\n",
      "OS: Linux 4.19.0-16-cloud-amd64\n",
      "multiprocessing available: True\n",
      "using cloud sdk: False\n",
      "pass cloud sdk credentials to gsutil: False\n",
      "config path(s): /etc/boto.cfg, /home/jupyter/.boto\n",
      "gsutil path: /opt/conda/bin/gsutil\n",
      "compiled crcmod: True\n",
      "installed via package manager: True\n",
      "editable install: False\n"
     ]
    }
   ],
   "source": [
    "!gsutil version -l"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab490742",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da8cc3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "      start_date: date object to start from.\n",
    "      end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "      date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def ingest_cds_to_gcs(single_date, bucket, cleanup=False):\n",
    "    \"\"\"Retrieve data from Copernicus Data Service and upload to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket: Google Cloud Storage bucket to upload to.\n",
    "        cleanup: Optionally remove the downloaded CDS data after upload to GCS.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; downloads a file, uploads it to GCS, and optionally removes it as side effects.\n",
    "    \"\"\"\n",
    "    c = cdsapi.Client(progress=False, quiet=True)\n",
    "    c.retrieve(\n",
    "    \"reanalysis-era5-single-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\n",
    "            \"surface_solar_radiation_downwards\", \"toa_incident_solar_radiation\",\n",
    "            \"surface_net_solar_radiation\", \"top_net_solar_radiation\"\n",
    "        ],\n",
    "        \"year\": single_date.strftime(\"%Y\"),\n",
    "        \"month\": single_date.strftime(\"%m\"),\n",
    "        \"day\": single_date.strftime(\"%d\"),\n",
    "        \"time\": [\n",
    "            \"00:00\", \"01:00\", \"02:00\",\n",
    "            \"03:00\", \"04:00\", \"05:00\",\n",
    "            \"06:00\", \"07:00\", \"08:00\",\n",
    "            \"09:00\", \"10:00\", \"11:00\",\n",
    "            \"12:00\", \"13:00\", \"14:00\",\n",
    "            \"15:00\", \"16:00\", \"17:00\",\n",
    "            \"18:00\", \"19:00\", \"20:00\",\n",
    "            \"21:00\", \"22:00\", \"23:00\",\n",
    "        ],\n",
    "        \"format\": \"netcdf\",\n",
    "    },\n",
    "    f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "    upload_status = system(f\"gsutil -m cp -r {single_date.strftime('%Y%m%d')}.nc gs://{bucket}/\")\n",
    "    if (upload_status == 0) & cleanup:\n",
    "        system(f\"rm {single_date.strftime('%Y%m%d')}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f46a45c",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The workflow is straightforward: for each day between the specified start and end dates, make a request for hourly data containing 4 variables, download the data, and upload it to Google Cloud Storage. `tqdm` tracks the progress of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1bc598",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8527fec4ee7d4daf8beb55b0d19c5461",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm_joblib(tqdm(total=sum(1 for _ in daterange(start_date, end_date)))) as pbar:\n",
    "    Parallel(n_jobs=-2,  # use all but one CPU\n",
    "             backend=\"multiprocessing\")(delayed(ingest_cds_to_gcs)(day, hourly_data_bucket, cleanup=True)\n",
    "                                        for day in daterange(start_date, end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefadef6",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In the next notebook, `02-Processing.ipynb` we'll demonstrate the steps necessary to distill the ingested hourly data into daily-mean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de18f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
