{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd0d04d7",
   "metadata": {},
   "source": [
    "# Data Ingest\n",
    "\n",
    "This notebook contains a workflow to:\n",
    "1. Download meteorological reanalysis data from the Copernicus [Climate Data Store](https://cds.climate.copernicus.eu/) and\n",
    "2. Upload the data to a Google Cloud Storage bucket.\n",
    "\n",
    "This data ingest is necessary to support an analysis of Earth's radiation budget, so we will request four variables to serve as boundary conditions:\n",
    "\n",
    "* Top of Atmosphere Incident Solar Radiation (`toa_incident_solar_radiation`)\n",
    "* Top of Atmosphere Net Solar Radiation (`top_net_solar_radiation`)\n",
    "* Downwelling Solar Radiation at the Surface (`surface_solar_radiation_downwards`)\n",
    "* Net Solar Radiation at the Surface (`surface_net_solar_radiation`)\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* A Copernicus Climate Data Store account ([Create new account](https://cds.climate.copernicus.eu/user/register))\n",
    "* A Google Cloud project with Cloud Storage enabled\n",
    "* The following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dccdcee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q cdsapi joblib tqdm urllib3 certifi google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d12f945",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1a2d129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import timedelta, date\n",
    "import logging\n",
    "from os import environ, path, system\n",
    "import sys\n",
    "\n",
    "import cdsapi\n",
    "import certifi\n",
    "import contextlib\n",
    "from google.cloud import storage\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm.notebook import tqdm\n",
    "import urllib3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d0a752",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Our analysis seeks a long-term estimate of the amount of outgoing radiation that Earth's surface can reflect. ERA5 has 42 years of hourly data available. A long-term climatology is typically defined as 30 years. Thus, we ingest the latest 30 years: 1991 through 2020. Since making a single request would be prohibitively large, we break the request up by day. \n",
    "\n",
    "Our ingest workflow downloads ECMWF Copernicus Climate Data Store (CDS) files locally, uploads them to a Google Cloud Storage bucket, and removes the local file. We define a GCS bucket here, although a user may use whatever bucket they choose in their Google Cloud project.\n",
    "\n",
    "The CDS Client API must be configured prior to use ([How to use the CDS API](https://cds.climate.copernicus.eu/api-how-to)). On initialization `cdsapi.Client()` looks for a `url` and `key` in environment variables, in a `.cdsapirc` file, or in the class input arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c0f7e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(1992, 12, 1)\n",
    "end_date = date(1993, 1, 1)\n",
    "hourly_data_bucket = \"era5-single-level\"\n",
    "n_jobs = -3  # number of jobs for parallelization; if 1, then serial; if negative, then (n_cpus + 1 + n_jobs) are used\n",
    "\n",
    "# ECMWF C3S CDS Client Configuration\n",
    "try:\n",
    "    if None not in (environ[\"CDSAPI_URL\"], environ[\"CDSAPI_KEY\"]):\n",
    "        pass\n",
    "except KeyError:\n",
    "    # Fallback to .cdsapirc\n",
    "    try:\n",
    "        with open(path.join(path.expanduser(\"~\"),\".cdsapirc\"), \"r\") as f:\n",
    "            output = f.read()\n",
    "    except IOError:\n",
    "        logging.warning(\"No $CDSAPI_URL, $CDSAPI_KEY, or .cdsapirc found.\")\n",
    "\n",
    "# Certificate management\n",
    "http = urllib3.PoolManager(\n",
    "    cert_reqs='CERT_REQUIRED',\n",
    "    ca_certs=certifi.where()\n",
    ")\n",
    "\n",
    "logging.basicConfig(filename=\"ingest.log\", filemode=\"w\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00a83ccf",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "44f2d288",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "      start_date: date object to start from.\n",
    "      end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "      date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "        \n",
    "\n",
    "def put_date_data_gcs(single_date, bucket_name):\n",
    "    \"\"\"Upload a dataset for a single date to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to download from.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; uploads data to Google Cloud Storage as side effect.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    blob.upload_from_filename(filename=f\"./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "\n",
    "def check_blob_size(single_date, bucket_name, raise_threshold=1):\n",
    "    \"\"\"Verify that a GCS blob is larger than a specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        raise_threshold: file size below which an exception should be raised.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.get_blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    if blob.size < raise_threshold:\n",
    "        raise Exception(f\"{single_date.strftime('%Y%m%d')} data file size is smaller than expected\")\n",
    "    else:\n",
    "        logging.info(f\"{single_date.strftime('%Y%m%d')} file size in GCS is {int(blob.size * 1e-6)}MB\")\n",
    "\n",
    "\n",
    "def ingest_cds_to_gcs(single_date, bucket_name, cleanup=False):\n",
    "    \"\"\"Retrieve data from Copernicus Data Service and upload data to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        cleanup: Optionally remove the downloaded CDS data after upload to GCS.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; downloads a file, uploads it to GCS, and removes it (optionally) as side effects.\n",
    "    \"\"\"\n",
    "    client = cdsapi.Client(progress=False, quiet=True)\n",
    "    client.retrieve(\n",
    "    \"reanalysis-era5-single-levels\",\n",
    "    {\n",
    "        \"product_type\": \"reanalysis\",\n",
    "        \"variable\": [\n",
    "            \"surface_solar_radiation_downwards\", \"toa_incident_solar_radiation\",\n",
    "            \"surface_net_solar_radiation\", \"top_net_solar_radiation\"\n",
    "        ],\n",
    "        \"year\": single_date.strftime(\"%Y\"),\n",
    "        \"month\": single_date.strftime(\"%m\"),\n",
    "        \"day\": single_date.strftime(\"%d\"),\n",
    "        \"time\": [\n",
    "            \"00:00\", \"01:00\", \"02:00\",\n",
    "            \"03:00\", \"04:00\", \"05:00\",\n",
    "            \"06:00\", \"07:00\", \"08:00\",\n",
    "            \"09:00\", \"10:00\", \"11:00\",\n",
    "            \"12:00\", \"13:00\", \"14:00\",\n",
    "            \"15:00\", \"16:00\", \"17:00\",\n",
    "            \"18:00\", \"19:00\", \"20:00\",\n",
    "            \"21:00\", \"22:00\", \"23:00\",\n",
    "        ],\n",
    "        \"format\": \"netcdf\",\n",
    "    },\n",
    "    f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "    put_date_data_gcs(single_date, bucket_name)\n",
    "    check_blob_size(single_date, bucket_name, raise_threshold=1.8e+8)\n",
    "    \n",
    "    if cleanup:\n",
    "        system(f\"rm {single_date.strftime('%Y%m%d')}.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4258d37",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "The workflow is straightforward: for each day between the specified start and end dates, make a request for hourly data containing 4 variables, download the data, and upload it to Google Cloud Storage. `tqdm` tracks the progress of the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7216a281",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c386f5a49d6b44b9bdae79faa11ca998",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "with tqdm_joblib(tqdm(total=sum(1 for _ in daterange(start_date, end_date)))) as pbar:\n",
    "    Parallel(n_jobs=n_jobs,\n",
    "             backend=\"multiprocessing\")(delayed(ingest_cds_to_gcs)(day, \n",
    "                                                                   hourly_data_bucket, \n",
    "                                                                   cleanup=True)\n",
    "                                        for day in daterange(start_date, end_date))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb8c9c13",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "In the next notebook, `02-Processing.ipynb` we'll demonstrate the steps necessary to distill the ingested hourly data into daily-mean data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095de11c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
