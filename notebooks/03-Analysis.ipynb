{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cbb10410-ea34-4e3c-9ddd-a84fdd93ee19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q tqdm xarray scipy netCDF4 joblib google-cloud-storage matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39f194be-4d63-4083-830e-5b10d5e295f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from datetime import timedelta, date\n",
    "import logging\n",
    "import multiprocessing\n",
    "from os import system\n",
    "from sys import platform\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "from google.cloud import storage\n",
    "from tqdm.notebook import tqdm\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c460627-f622-4dee-b6ab-25c9b6ab1439",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(1990, 1, 1)\n",
    "end_date = date(1990, 1, 1)\n",
    "daily_data_bucket = \"era5-single-level-daily\"\n",
    "n_jobs = -3  # number of jobs for parallelization; if 1, then serial; if negative, then (n_cpus + 1 + n_jobs) are used\n",
    "\n",
    "xr.set_options(keep_attrs=True)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "if platform == \"darwin\":\n",
    "    multiprocessing.set_start_method(\"fork\", force=True)  # ipython bug workaround https://github.com/ipython/ipython/issues/12396\n",
    "    \n",
    "logging.basicConfig(filename=\"analysis.log\", filemode=\"w\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "42e09678-76fc-464a-8ec5-8ad18c09c88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date: date object to start from.\n",
    "        end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "        date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def get_date_data_gcs(single_date, bucket_name):\n",
    "    \"\"\"Download a dataset for a single date from Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to download from.\n",
    "    \n",
    "    Returns:\n",
    "        Nothing; downloads data from Google Cloud Storage as a side effect.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    blob.download_to_filename(filename=f\"./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "\n",
    "\n",
    "def put_date_data_gcs(single_date, bucket_name):\n",
    "    \"\"\"Upload a dataset for a single date to Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to download from.\n",
    "        \n",
    "    Returns:\n",
    "        Nothing; uploads data to Google Cloud Storage as a side effect.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    blob.upload_from_filename(filename=f\"./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "\n",
    "def check_blob_size(single_date, bucket_name, raise_threshold=1):\n",
    "    \"\"\"Verify that a GCS blob is larger than a specified threshold.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to upload to.\n",
    "        raise_threshold: file size below which an exception should be raised.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.get_blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    if blob.size < raise_threshold:\n",
    "        raise Exception(f\"{single_date.strftime('%Y%m%d')} data file size is smaller than expected\")\n",
    "    else:\n",
    "        logging.info(f\"{single_date.strftime('%Y%m%d')} file size in GCS is {int(blob.size * 1e-6)}MB\")\n",
    "\n",
    "\n",
    "def modify_units(dataset, starting_units, ending_units, conversion_factor):\n",
    "    \"\"\"Modify the units of a variable.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset\n",
    "        starting_units: str of units to be modified\n",
    "        ending_units: str of units after modification\n",
    "        conversion_factor: numerical factor to apply to convert units\n",
    "    \n",
    "    Returns:\n",
    "        xarray Dataset with units modified for variables with units matching the starting unit.\n",
    "    \"\"\"\n",
    "    for variable in dataset:\n",
    "        if dataset[variable].attrs[\"units\"] == starting_units:\n",
    "            dataset[variable] = dataset[variable] * conversion_factor\n",
    "            dataset[variable].attrs[\"units\"] = ending_units\n",
    "    return dataset\n",
    "\n",
    "        \n",
    "def compute_daily_average(dataset):\n",
    "    \"\"\"Compute the daily average and an input xarray dataset.\"\"\"\n",
    "    return dataset.resample(time='1D').sum() / dataset.sizes[\"time\"]\n",
    "\n",
    "\n",
    "def compute_boundary_fluxes(dataset):\n",
    "    \"\"\"Compute missing boundary fluxes at the surface and top of atmosphere if possible.\n",
    "    \n",
    "    Use available radiative fluxes e.g. net solar radiation and incoming solar radiation to \n",
    "    compute outgoing solar radiation.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset with radiative fluxes at the surface and top of atmosphere.\n",
    "        \n",
    "    Returns:\n",
    "        xarray Dataset with missing fluxes at the boundaries.\n",
    "    \"\"\"\n",
    "    if (\"tosr\" not in dataset) and all(x in dataset for x in [\"tisr\", \"tsr\"]):\n",
    "        dataset[\"tosr\"] = dataset[\"tisr\"] - dataset[\"tsr\"]\n",
    "        dataset[\"tosr\"].attrs[\"long_name\"] = \"TOA outgoing solar radiation\"\n",
    "        dataset[\"tosr\"].attrs[\"standard_name\"] = \"toa_outgoing_shortwave_flux\"\n",
    "    if (\"ssru\" not in dataset) and all(x in dataset for x in [\"ssrd\", \"ssr\"]):\n",
    "        dataset[\"ssru\"] = dataset[\"ssrd\"] - dataset[\"ssr\"]\n",
    "        dataset[\"ssru\"].attrs[\"long_name\"] = \"Surface solar radiation upwards\"\n",
    "        dataset[\"ssru\"].attrs[\"standard_name\"] = \"surface_upwelling_shortwave_flux_in_air\"\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def compute_radiative_properties(dataset):\n",
    "    \"\"\"Compute new variables based on the model of Stephens et al. (2015).\"\"\"\n",
    "    # System properties\n",
    "    dataset[\"R\"] = dataset[\"tosr\"] / dataset[\"tisr\"]\n",
    "    dataset[\"R\"].attrs[\"long_name\"] = \"Planetary albedo\"\n",
    "    dataset[\"R\"].attrs[\"standard_name\"] = \"planetary_albedo\"\n",
    "    dataset[\"R\"].attrs[\"units\"] = \"1\"\n",
    "    \n",
    "    dataset[\"T\"] = dataset[\"ssrd\"] / dataset[\"tisr\"]\n",
    "    dataset[\"T\"].attrs[\"long_name\"] = \"Planetary transmission\"\n",
    "    dataset[\"T\"].attrs[\"standard_name\"] = \"planetary_transmittance\"\n",
    "    dataset[\"T\"].attrs[\"units\"] = \"1\"\n",
    "    \n",
    "    dataset[\"alpha\"] = dataset[\"ssru\"] / dataset[\"ssrd\"]\n",
    "    dataset[\"alpha\"].attrs[\"long_name\"] = \"Surface albedo\"\n",
    "    dataset[\"alpha\"].attrs[\"standard_name\"] = \"surface_albedo\"\n",
    "    dataset[\"alpha\"].attrs[\"units\"] = \"1\"\n",
    "\n",
    "    # Intrinsic properties\n",
    "    dataset[\"t\"] = ((1 - dataset[\"alpha\"] * dataset[\"R\"]) / \n",
    "                    (1 - dataset[\"alpha\"]**2 * dataset[\"T\"]**2))\n",
    "    dataset[\"t\"].attrs[\"long_name\"] = \"1-layer atmospheric transmission\"\n",
    "    dataset[\"t\"].attrs[\"standard_name\"] = \"atmosphere_transmittance\"\n",
    "    dataset[\"t\"].attrs[\"units\"] = \"1\"\n",
    "    \n",
    "    dataset[\"r\"] = dataset[\"R\"] - dataset[\"t\"] * dataset[\"alpha\"] * dataset[\"T\"]\n",
    "    dataset[\"r\"].attrs[\"long_name\"] = \"1-layer atmosphere reflectivity\"\n",
    "    dataset[\"r\"].attrs[\"standard_name\"] = \"atmosphere_reflectance\"\n",
    "    dataset[\"r\"].attrs[\"units\"] = \"1\"\n",
    "\n",
    "    # Reflective properties\n",
    "    dataset[\"srosr\"] = dataset[\"tisr\"] * (dataset[\"R\"] - dataset[\"r\"])\n",
    "    dataset[\"srosr\"].attrs[\"long_name\"] = \"Surface-reflected outgoing solar radiation\"\n",
    "    dataset[\"srosr\"].attrs[\"standard_name\"] = \"toa_outgoing_shortwave_flux\"\n",
    "    dataset[\"srosr\"].attrs[\"units\"] = \"J m**-2\"\n",
    "    \n",
    "    dataset[\"psrosr\"] = dataset[\"ssrd\"] * dataset[\"t\"]\n",
    "    dataset[\"psrosr\"].attrs[\"long_name\"] = \"Potential surface-reflected outgoing solar radiation\"\n",
    "    dataset[\"psrosr\"].attrs[\"standard_name\"] = \"toa_outgoing_shortwave_flux\"\n",
    "    dataset[\"psrosr\"].attrs[\"units\"] = \"J m**-2\"\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "\n",
    "def drop_unneccesary_variables(dataset, keep_vars):\n",
    "    \"\"\"Drop variables not specified as necessary.\n",
    "    \n",
    "    Args:\n",
    "        dataset: xarray Dataset.\n",
    "        keep_vars: list of variables to keep.\n",
    "    \n",
    "    Returns:\n",
    "        xarray Dataset with specified variables.\n",
    "    \"\"\"\n",
    "    drop_vars = list(set(dataset.data_vars).symmetric_difference(set(keep_vars)))\n",
    "    return dataset.drop_vars(drop_vars)\n",
    "\n",
    "\n",
    "def analyze_daily_data(single_date, raw_data_bucket, analyzed_data_bucket, cleanup=False):\n",
    "    \"\"\"Process hourly average data into daily average data.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        raw_data_bucket: str name of Google Cloud Storage bucket for raw daily data.\n",
    "        analyzed_data_bucket: str name of Google Cloud Storage bucket for analzyed daily data.\n",
    "        cleanup: boolean option to remove downloaded data after processing.\n",
    "        \n",
    "    Raises:\n",
    "        Exception: if the same bucket is provided for both raw and analyzed data.\n",
    "    \"\"\"\n",
    "    get_date_data_gcs(single_date, hourly_data_bucket)\n",
    "    \n",
    "    with xr.open_dataset(f\"./{single_date.strftime('%Y%m%d')}.nc\") as ds:\n",
    "        ds = compute_boundary_fluxes(ds)\n",
    "        ds = compute_radiative_properties(ds)\n",
    "        ds = drop_unneccesary_variables(ds, keep_vars=[\"ssrd\", \"ssru\", \"tisr\", \"tosr\", \n",
    "                                                       \"t\", \"r\", \"srosr\", \"psrosr\"])\n",
    "        ds = compute_daily_average(ds)\n",
    "        ds = modify_units(ds, \"J m**-2\", \"W m**-2\", (1 / 3600))  # 3600 seconds in an hour\n",
    "        ds.to_netcdf(f\"./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "\n",
    "    put_date_data_gcs(single_date, daily_data_bucket)\n",
    "    check_blob_size(single_date, daily_data_bucket, raise_threshold=1e+7)\n",
    "    \n",
    "    if cleanup:\n",
    "        system(f\"rm {single_date.strftime('%Y%m%d')}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d23e8459-9a0f-4785-b99d-f697ba7cd14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "get_date_data_gcs(start_date, daily_data_bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf56cd2-74ff-4a8c-9f96-a1c1e186ccf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(f\"./{start_date.strftime('%Y%m%d')}.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "252782b2-397a-4475-b792-bd93a38381de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
