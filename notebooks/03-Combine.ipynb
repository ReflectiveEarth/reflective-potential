{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d300a079",
   "metadata": {},
   "source": [
    "# Data Concatenation\n",
    "\n",
    "This notebook contains a workflow to:\n",
    "1. Download daily ERA5 data from a Google Cloud Storage bucket,\n",
    "1. Combine the daily data in a Zarr store, and\n",
    "1. Upload the Zarr store to a Google Cloud Storage bucket.\n",
    "\n",
    "This step is a convenience that facilitaties easier sharing via the Zarr format for chunked, compressed, N-D arrays. In this way, we follow the lead of the [Pangeo project](https://pangeo.io/). They also [store data in the cloud](https://pangeo.io/data.html) using the Zarr format.\n",
    "\n",
    "## Preliminaries\n",
    "\n",
    "### Requirements\n",
    "\n",
    "* A Google Cloud project with Cloud Storage enabled ([Create new account](https://cloud.google.com/))\n",
    "* 300 GB of local storage for data\n",
    "* The following Python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19d583c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -q tqdm xarray dask scipy netCDF4 zarr google-cloud-storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ee1bbfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import contextlib\n",
    "from datetime import timedelta, date\n",
    "import logging\n",
    "import multiprocessing\n",
    "from os import system, path\n",
    "from sys import platform\n",
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "from google.cloud import storage\n",
    "from tqdm.notebook import tqdm, trange\n",
    "import joblib\n",
    "from joblib import Parallel, delayed\n",
    "import xarray as xr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d453921e",
   "metadata": {},
   "source": [
    "### Setup\n",
    "\n",
    "Our start and end dates span 30 years. Daily data are maintained in a Google Cloud Storage bucket separate from the Zarr store we seek to create. We also need to specify a local path to the Zarr store produced by the workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "643fa355",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = date(1991, 1, 1)\n",
    "end_date = date(2021, 1, 1)\n",
    "daily_data_bucket = \"era5-single-level-daily\"\n",
    "local_path_to_store = \"./era5-daily.zarr\"\n",
    "zarr_bucket = \"rom-input\"\n",
    "n_jobs = -3  # number of jobs for parallelization; if 1, then serial; if negative, then (n_cpus + 1 + n_jobs) are used\n",
    "\n",
    "# Xarray configuration\n",
    "xr.set_options(keep_attrs=True)\n",
    "\n",
    "# Multiprocessing configuration for MacOS\n",
    "if platform == \"darwin\":\n",
    "    multiprocessing.set_start_method(\"fork\", force=True)  # ipython bug workaround https://github.com/ipython/ipython/issues/12396\n",
    "    \n",
    "# Logging configuration\n",
    "logging.basicConfig(filename=\"combine.log\", filemode=\"w\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "370d0bd3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f45919",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextlib.contextmanager\n",
    "def tqdm_joblib(tqdm_object):\n",
    "    \"\"\"Patch joblib to report into tqdm progress bar given as argument.\"\"\"\n",
    "\n",
    "    def tqdm_print_progress(self):\n",
    "        if self.n_completed_tasks > tqdm_object.n:\n",
    "            n_completed = self.n_completed_tasks - tqdm_object.n\n",
    "            tqdm_object.update(n=n_completed)\n",
    "\n",
    "    original_print_progress = joblib.parallel.Parallel.print_progress\n",
    "    joblib.parallel.Parallel.print_progress = tqdm_print_progress\n",
    "\n",
    "    try:\n",
    "        yield tqdm_object\n",
    "    finally:\n",
    "        joblib.parallel.Parallel.print_progress = original_print_progress\n",
    "        tqdm_object.close()\n",
    "\n",
    "\n",
    "def daterange(start_date, end_date):\n",
    "    \"\"\"Make a date range object spanning two dates.\n",
    "    \n",
    "    Args:\n",
    "        start_date: date object to start from.\n",
    "        end_date: date object to end at.\n",
    "    \n",
    "    Yields:\n",
    "        date object for iteration.\n",
    "    \"\"\"\n",
    "    for n in range(int ((end_date - start_date).days)):\n",
    "        yield start_date + timedelta(n)\n",
    "\n",
    "\n",
    "def get_date_data_gcs(single_date, bucket_name):\n",
    "    \"\"\"Download a dataset for a single date from Google Cloud Storage.\n",
    "    \n",
    "    Args:\n",
    "        single_date: date object representing day to retrieve data for.\n",
    "        bucket_name: Google Cloud Storage bucket to download from.\n",
    "    \n",
    "    Returns:\n",
    "        Nothing; downloads data from Google Cloud Storage as a side effect.\n",
    "    \"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.get_bucket(bucket_name)    \n",
    "    blob = bucket.blob(f\"{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    blob.download_to_filename(filename=f\"./{single_date.strftime('%Y%m%d')}.nc\")\n",
    "    \n",
    "\n",
    "def combine_data(path_to_data, path_to_store, start_year, end_year):\n",
    "    \"\"\"Combine daily data files into a single store.\"\"\"\n",
    "    for year in trange(start_year, end_year):\n",
    "        \n",
    "        files = [f\"{path_to_data}/{single_date.strftime('%Y%m%d')}.nc\" \n",
    "                 for single_date in daterange(date(year, 1, 1), date(year + 1, 1, 1))]\n",
    "        ds = xr.open_mfdataset(files, parallel=True)\n",
    "        ds.attrs[\"institution\"] = \"ECMWF\"\n",
    "        ds.attrs[\"source\"] = \"ERA5\"\n",
    "        ds.attrs[\"title\"] = \"Reflective Earth optimization map inputs\"\n",
    "        ds.attrs[\"comment\"] = \"Hourly-mean ERA5 boundary solar radiation fields were averaged into daily-means\"\n",
    "        \n",
    "        if path.exists(path_to_store):\n",
    "            delayed_store = ds.to_zarr(path_to_store, consolidated=True, compute=False, append_dim=\"time\")\n",
    "        else:\n",
    "            delayed_store = ds.to_zarr(path_to_store, consolidated=True, compute=False)\n",
    "        \n",
    "        with ProgressBar():\n",
    "             results = delayed_store.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ddb95e",
   "metadata": {},
   "source": [
    "## Workflow\n",
    "\n",
    "First we need to download all of the daily data files to a local directory, then we combine the data into a single Zarr store, and finally upload it to Google Cloud storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f8de626",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm_joblib(tqdm(total=sum(1 for _ in daterange(start_date, end_date)))) as pbar:\n",
    "    Parallel(n_jobs=n_jobs,\n",
    "             backend=\"multiprocessing\")(delayed(get_date_data_gcs)(day, daily_data_bucket)\n",
    "                                        for day in daterange(start_date, end_date))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2afd89f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_data(path_to_data=\".\",\n",
    "             path_to_store=local_path_to_store,\n",
    "             start_year=start_date.year, end_year=end_date.year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f65c1208",
   "metadata": {},
   "outputs": [],
   "source": [
    "system(f\"gsutil -m cp -r {local_path_to_store} gs://{zarr_bucket}/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845f38d",
   "metadata": {},
   "source": [
    "## Discussion\n",
    "\n",
    "Now that we have combined daily ERA5 data into a Zarr store, we can analyze the data using a simple model of reflected solar radiation. Look for this in the next notebook, `04-Analyze.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62bb46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "common-cpu.m71",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/base-cpu:m71"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
